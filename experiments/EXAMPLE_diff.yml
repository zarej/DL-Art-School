# Notes:
#### general settings
name: CHANGEME_your_experiment_name
model: extensibletrainer
distortion: sr  # TODO: ????????
scale: 1
gpu_ids: [0] # <-- unless you have multiple gpus, use this
start_step: 0 # -1 causes 0.pth to be saved!
#force_start_step: 39000
checkpointing_enabled: true  # <-- Gradient checkpointing. Enable for huge GPU memory savings. Disable for distributed training.
grad_scaler_enabled: false # TODO: ????????
fp16: false # TODO: why does enabling this with 8bit slow down perf??
use_8bit: true
use_tb_logger: true
wandb: false  # <-- enable to log to wandb. tensorboard logging is always enabled.

# diffusion doesn't actually need paired voice labels...
# HOWEVER, all of the other data fetching modes don't actually provide 'wav' outputs in the dict.
# Possibly could be made more efficient in the future.
datasets:
  train:
    name: TrainDS
    n_workers: 4 # idk what this does
    batch_size: 32 # gives ~16GB on my 3090
    mode: paired_voice_audio
    sample_rate: 22050
    path: CHANGEME_training_dataset_name
    fetcher_mode: ['lj'] # CHANGEME if your dataset isn't in LJSpeech format
    max_wav_length: 220500 # WAS: 255995 
    max_text_length: 999 # WAS: 200
    load_conditioning: true # note that the diffuser doesn't use this, but the gpt model that passes outputs to the diffuser does.
    num_conditioning_candidates: 1 # WAS: 2
    conditioning_length: 102400 # WAS 44000
    needs_collate: false # TODO: ??????????
    use_bpe_tokenizer: true
    load_aligned_codes: false
    phase: train
  val:
    name: TestDS
    n_workers: 2
    batch_size: 16 # this could be higher probably
    mode: paired_voice_audio
    sample_rate: 22050
    path: CHANGEME_validation_dataset_name
    fetcher_mode: ['lj']
    max_wav_length: 220500 # WAS: 255995 
    max_text_length: 999 # WAS: 200
    load_conditioning: true
    num_conditioning_candidates: 1 # WAS 2
    conditioning_length: 102400 # WAS 44000
    needs_collate: false # TODO: ??????????
    use_bpe_tokenizer: true
    load_aligned_codes: false
    phase: val


steps:
  ddpm_train:
    training: ddpm
    loss_log_buffer: 2000 #????
    step_outputs: [loss]

    optimizer: adamw
    #optimizer: lion
    optimizer_params:
      lr: !!float 1e-5 # TODO: change this to something more reasonable
      #lr: !!float 2e-6 # USE LOWER LR for LION
      triton: false # ONLY RELEVANT FOR LION
      weight_decay: 0.001
      beta1: 0.9
      beta2: 0.999 # ??? CHANGEME this is 0.98 elsewhere
    clip_grad_eps: 1.0 # ??? CHANGEME this is 5.0 elsewhere

    injectors:
      to_mel:
        type: torch_mel_spectrogram
        mel_norm_file: ../experiments/clips_mel_norms.pth
        in: wav
        out: mel
      resample_wav:
        type: audio_resample
        in: wav
        out: wav_for_vocoder
        input_sample_rate: 22050
        output_sample_rate: 24000
      tacotron_mel:
        type: mel_spectrogram
        mel_fmax: 12000
        sampling_rate: 24000
        n_mel_channels: 100
        # Only normalize the MEL target, because the diffuser specifically cares about it.
        do_normalization: true
        in: wav_for_vocoder
        out: target_mel
      resample_cond:
        type: for_each
        subtype: audio_resample
        input_sample_rate: 22050
        output_sample_rate: 24000
        in: conditioning
        out: conditioning_for_vocoder
      cond_to_mel:
        type: for_each
        subtype: mel_spectrogram
        mel_fmax: 12000
        sampling_rate: 24000
        n_mel_channels: 100
        in: conditioning_for_vocoder
        out: cond_mel
      produce_latents:
        type: gpt_voice_latent
        gpt_path: CHANGEME_path_to_finetuned_gpt_checkpoint #../experiments/finetune_gpt_unified_large_kennedy/models/800_gpt_ema.pth
        in: wav
        conditioning_clip: conditioning
        text: padded_text
        text_lengths: text_lengths
        input_lengths: wav_lengths
        out: gpt_latent
      diffusion:
        type: gaussian_diffusion
        in: target_mel
        generator: ddpm
        beta_schedule:
          schedule_name: linear
          num_diffusion_timesteps: 4000 # :(
        diffusion_args:
          model_mean_type: epsilon
          model_var_type: learned_range
          loss_type: mse
        sampler_type: uniform
        model_input_keys:
          aligned_conditioning: gpt_latent
          conditioning_input: cond_mel
          return_code_pred: true
        extra_model_output_keys: [mel_pred]
        out: loss
        out_key_vb_loss: vb_loss
        out_key_x_start: x_start_pred
    losses:
      diffusion_loss:
        after: 200 # TODO: figure out what the right number for this is
        type: direct
        weight: 1
        key: loss
      var_loss:
        after: 200
        type: direct
        weight: 1
        key: vb_loss
      mel_surrogate: # TODO: figure out if this is useful at all
        type: pix
        weight: 1
        criterion: l2
        real: target_mel
        fake: mel_pred

networks:
  ddpm:
    type: generator 
    which_model_G: diffusion_tts_flat # https://github.com/neonbjb/DL-Art-School/issues/5#issuecomment-1142280204
    kwargs:
      model_channels: 1024
      num_layers: 10
      in_channels: 100
      out_channels: 200
      in_latent_channels: 1024
      in_tokens: 8193
      dropout: 0 # could be 0.1?
      use_fp16: false
      num_heads: 16
      layer_drop: 0 # not even 0.1 in default
      unconditioned_percentage: 0.1 # CHANGEME might be better to be 0!
     #freeze_everything_except_autoregressive_inputs: False # probably never a good thing???

path:
  pretrain_model_gpt: '~/.cache/tortoise/models/diffusion_decoder.pth' # CHANGEME: copy this from tortoise cache
  strict_load: true
  #resume_state: ../experiments/train_imgnet_vqvae_stage1/training_state/0.state   # <-- Set this to resume from a previous training state.

# CHANGEME: ALL OF THESE PARAMETERS SHOULD BE EXPERIMENTED WITH
# afaik all units here are measured in **steps** (i.e. one batch of batch_size is 1 unit)
train:
  niter: 50000
  warmup_iter: -1 # could be 0???
  warmup_steps: 0 # TODO ????????
  mega_batch_factor: 8    # <-- Gradient accumulation factor. If you are running OOM, increase this to [2,4,8].
  val_freq: 150 # Personally I believe this should be set to epoch size * 3
  sort_key: wav_lengths # TODO: ????
  auto_collate: true # TODO: ????

  default_lr_scheme: MultiStepLR
  gen_lr_steps: [ 99999999 ] # TODO ??? [500, 1000, 1400, 1800] #[50000, 100000, 140000, 180000]
  lr_gamma: 0.2 # TODO ??? 0.5
  ema_enabled: false
  #ema_rate: .9995
  #ema_on_cpu: true
  #manual_seed: 1337 # add this if you want reproducibility

eval:
  pure: true # see train.py
# evaluators: TODO
#   fid:
#     for: generator
#     type: audio_diffusion_fid
#     eval_tsv: /y/libritts/test-clean/transcribed-brief-w2v.tsv
#     diffusion_steps: 50
#     diffusion_schedule: linear
#     diffusion_type: tts9_mel_autoin
#     conditioning_free: false
#     conditioning_free_k: 1

logger:
  print_freq: 50 # TODO: set this to epoch size
  save_checkpoint_freq: 150 # Personally I think this should be the same as val_freq
  visuals: [gen, mel] # idk
  visual_debug_rate: 99999999 # still not using this yet
  is_mel_spectrogram: true
  disable_state_saving: true # CHANGEME if you plan to halt training inbetween

upgrades:
  # Variable: number_of_checkpoints_to_save
  # Description: Define how many checkpoints should be saved on disk (1 checkpoint = pth+ =~ 6.8 GB)
  # Type: integer
  # Value: should be the same value as for number_of_states_to_save
  # smaller than 1 - turn off this option; there is no max value. For Colab use 1 or 2.
  # For Colab use 1 or 2 for gDrive and 5 for instance drive
  # 1 == Leave last saved checkpoint + last saved state (about 6.8 GB).
  # 2 == Leave last 2 saved checkpoints + last saved states (about 2 *~ 6.8 GB =~ 13.6 GB).
  number_of_checkpoints_to_save: 0
  # Variable: number_of_states_to_save
  # Description: Define how many states should be saved on disk (1 state =~ 3.4 GB)
  # if disable_state_saving is set as true this option will be inactive
  # Type: integer
  # Value: should be the same value as for number_of_checkpoints_to_save
  # smaller than 1 - turn off this option; there is no max value.
  # For Colab use 1 or 2 for gDrive and 5 for instance drive
  # 1 == Leave last saved state (about 3.4 GB).
  # 2 == Leave last 2 saved states (about 2 *~ 3.4 GB =~ 6.8 GB).
  number_of_states_to_save: 0
